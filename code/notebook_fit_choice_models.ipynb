{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian Logistic Regression using PyStan\n",
    "**Florian Ott, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit the models which explained behaviour best within their strategy class: Planning (PM), Simple (SM) and Hybrid (HM). Further explanation of the models, model comparison results and information about model validation can be found in the main manuscript.                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import arviz as az\n",
    "import glob as glob\n",
    "import time as time\n",
    "\n",
    "# Load particpant data \n",
    "filename = glob.glob('../data/behaviour/data_all_participants_20220215120148.csv') \n",
    "dat = pd.read_csv(filename[0],index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m01 = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0,upper=1> response[N];\n",
    "  vector[N] is_basic_1;\n",
    "  vector[N] is_basic_2;\n",
    "  vector[N] is_basic_3;\n",
    "  vector[N] is_basic_4;\n",
    "  vector[N] is_full_energy;\n",
    "  vector[N] is_low_energy_LC;\n",
    "  vector[N] is_low_energy_HC;\n",
    "  int<lower=0> N_subjects;\n",
    "  int<lower = 1> vpn[N];  \n",
    "}\n",
    "\n",
    "parameters {\n",
    "// hyper paramters \n",
    "  real mu_theta_basic_1;\n",
    "  real mu_theta_basic_2;\n",
    "  real mu_theta_basic_3;\n",
    "  real mu_theta_basic_4;\n",
    "  real<lower=0> sigma_theta_basic_1;\n",
    "  real<lower=0> sigma_theta_basic_2;\n",
    "  real<lower=0> sigma_theta_basic_3;\n",
    "  real<lower=0> sigma_theta_basic_4;\n",
    "\n",
    "// parameters\n",
    "  vector[N_subjects] theta_basic_1;\n",
    "  vector[N_subjects] theta_basic_2;\n",
    "  vector[N_subjects] theta_basic_3;\n",
    "  vector[N_subjects] theta_basic_4;\n",
    "  real theta_full_energy;\n",
    "  real theta_low_energy_LC;\n",
    "  real theta_low_energy_HC;\n",
    "}\n",
    "\n",
    "model {\n",
    "//hyper priors\n",
    "  mu_theta_basic_1 ~ normal(0,2);\n",
    "  mu_theta_basic_2 ~ normal(0,2);\n",
    "  mu_theta_basic_3 ~ normal(0,2);\n",
    "  mu_theta_basic_4 ~ normal(0,2);\n",
    "  sigma_theta_basic_1 ~ normal(0,2);\n",
    "  sigma_theta_basic_2 ~ normal(0,2);\n",
    "  sigma_theta_basic_3 ~ normal(0,2);\n",
    "  sigma_theta_basic_4 ~ normal(0,2);\n",
    "\n",
    "// priors\n",
    "  theta_basic_1 ~ normal(mu_theta_basic_1,sigma_theta_basic_1);\n",
    "  theta_basic_2 ~ normal(mu_theta_basic_2,sigma_theta_basic_2);\n",
    "  theta_basic_3 ~ normal(mu_theta_basic_3,sigma_theta_basic_3);\n",
    "  theta_basic_4 ~ normal(mu_theta_basic_4,sigma_theta_basic_4);\n",
    "  theta_full_energy ~ normal(0,2);\n",
    "  theta_low_energy_LC ~ normal(0,2);\n",
    "  theta_low_energy_HC ~ normal(0,2);\n",
    "\n",
    "// likelihood \n",
    "  response ~ bernoulli_logit(theta_full_energy * is_full_energy + theta_low_energy_LC * is_low_energy_LC + theta_low_energy_HC * is_low_energy_HC + theta_basic_1[vpn] .* is_basic_1 + theta_basic_2[vpn] .* is_basic_2 + theta_basic_3[vpn] .* is_basic_3 + theta_basic_4[vpn] .* is_basic_4);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  vector[N] response_new;\n",
    "  vector[N_subjects] theta_basic_1_rep;\n",
    "  vector[N_subjects] theta_basic_2_rep;\n",
    "  vector[N_subjects] theta_basic_3_rep;\n",
    "  vector[N_subjects] theta_basic_4_rep;\n",
    "\n",
    "\n",
    "// pointwise log-likelihood\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = bernoulli_logit_lpmf(response[n]  |  (theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1[vpn[n]] * is_basic_1[n] + theta_basic_2[vpn[n]] * is_basic_2[n] + theta_basic_3[vpn[n]] * is_basic_3[n] + theta_basic_4[vpn[n]] * is_basic_4[n]));\n",
    "    }\n",
    "\n",
    "// posterior predictive simulation  \n",
    "  for (n in 1:N_subjects){\n",
    "    theta_basic_1_rep[n] = normal_rng(mu_theta_basic_1, sigma_theta_basic_1);\n",
    "    theta_basic_2_rep[n] = normal_rng(mu_theta_basic_2, sigma_theta_basic_2);\n",
    "    theta_basic_3_rep[n] = normal_rng(mu_theta_basic_3, sigma_theta_basic_3);\n",
    "    theta_basic_4_rep[n] = normal_rng(mu_theta_basic_4, sigma_theta_basic_4);\n",
    "    }  \n",
    "\n",
    "  for (n in 1:N){\n",
    "    response_new[n] = bernoulli_logit_rng(theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1_rep[vpn[n]] * is_basic_1[n] + theta_basic_2_rep[vpn[n]] * is_basic_2[n] + theta_basic_3_rep[vpn[n]] * is_basic_3[n] + theta_basic_4_rep[vpn[n]] * is_basic_4[n]);\n",
    "    } \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_e4ff3fb425a2014d4aad18ac4f9e5dff NOW.\n"
     ]
    }
   ],
   "source": [
    "sm01 = pystan.StanModel(model_code=m01,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_1 = dat.loc[idx,['is_basic_1']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_2 = dat.loc[idx,['is_basic_2']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_3 = dat.loc[idx,['is_basic_3']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_4 = dat.loc[idx,['is_basic_4']].to_numpy(dtype='int').squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "\n",
    "dat_dict = {'N':len(response),         \n",
    "            'response':response,\n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic_1':is_basic_1,\n",
    "            'is_basic_2':is_basic_2,\n",
    "            'is_basic_3':is_basic_3,\n",
    "            'is_basic_4':is_basic_4,\n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "res_sm01 = sm01.sampling(data=dat_dict, iter=2000,  warmup=1000, thin=1, chains=4,control = dict(adapt_delta=0.99),seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing leave-one-out cross-validation information criterion (LOOIC) for model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\stan\\lib\\site-packages\\arviz\\stats\\stats.py:532: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n"
     ]
    }
   ],
   "source": [
    "idata_sm01 = az.from_pystan(posterior=res_sm01,log_likelihood='log_lik');\n",
    "looic_sm01 = az.loo(idata_sm01,pointwise=True,scale='deviance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m02 = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0,upper=1> response[N];\n",
    "  vector[N] dv;\n",
    "  vector[N] is_basic;\n",
    "  vector[N] is_full_energy;\n",
    "  vector[N] is_low_energy_LC;\n",
    "  vector[N] is_low_energy_HC;\n",
    "  int<lower=0> N_subjects;\n",
    "  int<lower = 1> vpn[N];  \n",
    "}\n",
    "\n",
    "parameters {\n",
    "//hyper parameters\n",
    "  real mu_theta_basic;\n",
    "  real mu_beta_dv;\n",
    "  real<lower=0> sigma_theta_basic;\n",
    "  real<lower=0> sigma_beta_dv;\n",
    "  \n",
    "//parameters \n",
    "  vector[N_subjects] theta_basic;\n",
    "  real theta_full_energy;\n",
    "  real theta_low_energy_LC;\n",
    "  real theta_low_energy_HC;\n",
    "  vector[N_subjects] beta_dv;\n",
    "}\n",
    "\n",
    "model {\n",
    "//hyper priors\n",
    "  mu_theta_basic ~ normal(0,2);\n",
    "  mu_beta_dv ~ normal(0,2);\n",
    "  sigma_theta_basic ~ normal(0,2);\n",
    "  sigma_beta_dv ~ normal(0,2);\n",
    "\n",
    "// priors \n",
    "  theta_basic ~ normal(mu_theta_basic, sigma_theta_basic);\n",
    "  theta_full_energy ~ normal(0, 2);\n",
    "  theta_low_energy_LC ~ normal(0, 2);\n",
    "  theta_low_energy_HC ~ normal(0, 2);\n",
    "  beta_dv ~ normal(mu_beta_dv,sigma_beta_dv);  \n",
    "\n",
    "// likelihood \n",
    "  response ~ bernoulli_logit(theta_full_energy * is_full_energy + theta_low_energy_LC * is_low_energy_LC + theta_low_energy_HC * is_low_energy_HC + theta_basic[vpn] .* is_basic + beta_dv[vpn] .* dv);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  vector[N] response_new;\n",
    "  vector[N_subjects] theta_basic_rep;\n",
    "  vector[N_subjects] beta_dv_rep;\n",
    "\n",
    "// pointwise log-likelihood\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = bernoulli_logit_lpmf(response[n]  |  (theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic[vpn[n]] * is_basic[n] + beta_dv[vpn[n]] * dv[n]));\n",
    "    }\n",
    "\n",
    "// posterior predictive simulation  \n",
    "  for (n in 1:N_subjects){\n",
    "    theta_basic_rep[n] = normal_rng(mu_theta_basic, sigma_theta_basic);\n",
    "    beta_dv_rep[n] = normal_rng(mu_beta_dv, sigma_beta_dv);\n",
    "    }\n",
    "\n",
    "  for (n in 1:N){\n",
    "    response_new[n] = bernoulli_logit_rng(theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_rep[vpn[n]] * is_basic[n] + beta_dv_rep[vpn[n]] * dv[n]);\n",
    "    } \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_430bf6b44e78bc5e7461815b80fab04b NOW.\n"
     ]
    }
   ],
   "source": [
    "sm02 = pystan.StanModel(model_code=m02,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic = dat.loc[idx,['is_basic']].to_numpy(dtype='int').squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "dv = dat.loc[idx,['dv_planning']].to_numpy().squeeze()\n",
    "\n",
    "dat_dict = {'N':len(response),         \n",
    "            'response':response,\n",
    "            'dv':dv,      \n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic':is_basic,\n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "res_sm02 = sm02.sampling(data=dat_dict, iter=2000,  warmup=1000, thin=1, chains=4,control = dict(adapt_delta=0.95),seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing leave-one-out cross-validation information criterion (LOOIC) for model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_sm02 = az.from_pystan(posterior=res_sm02,log_likelihood='log_lik');\n",
    "looic_sm02 = az.loo(idata_sm02,pointwise=True,scale='deviance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m03 = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0,upper=1> response[N];\n",
    "  vector[N] dv;\n",
    "  vector[N] is_basic_1;\n",
    "  vector[N] is_basic_2;\n",
    "  vector[N] is_basic_3;\n",
    "  vector[N] is_basic_4;\n",
    "  vector[N] is_23;\n",
    "  vector[N] is_14;\n",
    "  vector[N] is_full_energy;\n",
    "  vector[N] is_low_energy_LC;\n",
    "  vector[N] is_low_energy_HC;\n",
    "  int<lower=0> N_subjects;\n",
    "  int<lower = 1> vpn[N];  \n",
    "}\n",
    "\n",
    "parameters {\n",
    "// hyper paramters \n",
    "  real mu_theta_basic_1;\n",
    "  real mu_theta_basic_2;\n",
    "  real mu_theta_basic_3;\n",
    "  real mu_theta_basic_4;\n",
    "  real mu_beta_dv_23;  \n",
    "  real mu_beta_dv_14;  \n",
    "  real<lower=0> sigma_theta_basic_1;\n",
    "  real<lower=0> sigma_theta_basic_2;\n",
    "  real<lower=0> sigma_theta_basic_3;\n",
    "  real<lower=0> sigma_theta_basic_4;\n",
    "  real<lower=0> sigma_beta_dv_23;\n",
    "  real<lower=0> sigma_beta_dv_14;\n",
    "\n",
    "\n",
    "// parameters\n",
    "  vector[N_subjects] theta_basic_1;\n",
    "  vector[N_subjects] theta_basic_2;\n",
    "  vector[N_subjects] theta_basic_3;\n",
    "  vector[N_subjects] theta_basic_4;\n",
    "  real theta_full_energy;\n",
    "  real theta_low_energy_LC;\n",
    "  real theta_low_energy_HC;\n",
    "  vector[N_subjects] beta_dv_23;\n",
    "  vector[N_subjects] beta_dv_14;\n",
    "\n",
    "}\n",
    "\n",
    "model {\n",
    "//hyper priors\n",
    "  mu_theta_basic_1 ~ normal(0,2);\n",
    "  mu_theta_basic_2 ~ normal(0,2);\n",
    "  mu_theta_basic_3 ~ normal(0,2);\n",
    "  mu_theta_basic_4 ~ normal(0,2);\n",
    "  mu_beta_dv_23 ~ normal(0,2);\n",
    "  mu_beta_dv_14 ~ normal(0,2);  \n",
    "  sigma_theta_basic_1 ~ normal(0,2);\n",
    "  sigma_theta_basic_2 ~ normal(0,2);\n",
    "  sigma_theta_basic_3 ~ normal(0,2);\n",
    "  sigma_theta_basic_4 ~ normal(0,2);\n",
    "  sigma_beta_dv_23 ~ normal(0,2);\n",
    "  sigma_beta_dv_14 ~ normal(0,2);\n",
    "\n",
    "// priors\n",
    "  theta_basic_1 ~ normal(mu_theta_basic_1,sigma_theta_basic_1);\n",
    "  theta_basic_2 ~ normal(mu_theta_basic_2,sigma_theta_basic_2);\n",
    "  theta_basic_3 ~ normal(mu_theta_basic_3,sigma_theta_basic_3);\n",
    "  theta_basic_4 ~ normal(mu_theta_basic_4,sigma_theta_basic_4);\n",
    "  theta_full_energy ~ normal(0,2);\n",
    "  theta_low_energy_LC ~ normal(0,2);\n",
    "  theta_low_energy_HC ~ normal(0,2);\n",
    "  beta_dv_23 ~ normal(mu_beta_dv_23,sigma_beta_dv_23);  \n",
    "  beta_dv_14 ~ normal(mu_beta_dv_14,sigma_beta_dv_14);  \n",
    "\n",
    "// likelihood \n",
    "  response ~ bernoulli_logit(theta_full_energy * is_full_energy + theta_low_energy_LC * is_low_energy_LC + theta_low_energy_HC * is_low_energy_HC + theta_basic_1[vpn] .* is_basic_1 + theta_basic_2[vpn] .* is_basic_2 + theta_basic_3[vpn] .* is_basic_3 + theta_basic_4[vpn] .* is_basic_4 + beta_dv_23[vpn] .* dv .* is_23 + beta_dv_14[vpn] .* dv .* is_14);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  vector[N] response_new;\n",
    "  vector[N_subjects] theta_basic_1_rep;\n",
    "  vector[N_subjects] theta_basic_2_rep;\n",
    "  vector[N_subjects] theta_basic_3_rep;\n",
    "  vector[N_subjects] theta_basic_4_rep;\n",
    "  vector[N_subjects] beta_dv_rep_23;\n",
    "  vector[N_subjects] beta_dv_rep_14;\n",
    "\n",
    "\n",
    "// pointwise log-likelihood\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = bernoulli_logit_lpmf(response[n]  |  (theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1[vpn[n]] * is_basic_1[n] + theta_basic_2[vpn[n]] * is_basic_2[n] + theta_basic_3[vpn[n]] * is_basic_3[n] + theta_basic_4[vpn[n]] * is_basic_4[n] + beta_dv_23[vpn[n]] * dv[n] * is_23[n] + beta_dv_14[vpn[n]] * dv[n] * is_14[n]));\n",
    "    }\n",
    "\n",
    "// posterior predictive simulation  \n",
    "  for (n in 1:N_subjects){\n",
    "    theta_basic_1_rep[n] = normal_rng(mu_theta_basic_1, sigma_theta_basic_1);\n",
    "    theta_basic_2_rep[n] = normal_rng(mu_theta_basic_2, sigma_theta_basic_2);\n",
    "    theta_basic_3_rep[n] = normal_rng(mu_theta_basic_3, sigma_theta_basic_3);\n",
    "    theta_basic_4_rep[n] = normal_rng(mu_theta_basic_4, sigma_theta_basic_4);\n",
    "    beta_dv_rep_23[n] = normal_rng(mu_beta_dv_23, sigma_beta_dv_23);\n",
    "    beta_dv_rep_14[n] = normal_rng(mu_beta_dv_14, sigma_beta_dv_14);\n",
    "\n",
    "    }  \n",
    "\n",
    "  for (n in 1:N){\n",
    "    response_new[n] = bernoulli_logit_rng(theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1_rep[vpn[n]] * is_basic_1[n] + theta_basic_2_rep[vpn[n]] * is_basic_2[n] + theta_basic_3_rep[vpn[n]] * is_basic_3[n] + theta_basic_4_rep[vpn[n]] * is_basic_4[n] + beta_dv_rep_23[vpn[n]] * dv[n] * is_23[n] + beta_dv_rep_14[vpn[n]] * dv[n] * is_14[n]);\n",
    "    } \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_e96711c7616c1e7d3db6e6183c21a3cb NOW.\n"
     ]
    }
   ],
   "source": [
    "sm03 = pystan.StanModel(model_code=m03,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_1 = dat.loc[idx,['is_basic_1']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_2 = dat.loc[idx,['is_basic_2']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_3 = dat.loc[idx,['is_basic_3']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_4 = dat.loc[idx,['is_basic_4']].to_numpy(dtype='int').squeeze()\n",
    "is_14 = dat.loc[idx,['is_14']].to_numpy(dtype='int').squeeze()\n",
    "is_23 = dat.loc[idx,['is_23']].to_numpy(dtype='int').squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "dv = dat.loc[idx,['dv_planning']].to_numpy().squeeze()\n",
    "\n",
    "dat_dict  = {'N':len(response),         \n",
    "            'response':response,\n",
    "            'dv':dv,      \n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic_1':is_basic_1,\n",
    "            'is_basic_2':is_basic_2,\n",
    "            'is_basic_3':is_basic_3,\n",
    "            'is_basic_4':is_basic_4,\n",
    "            'is_23':is_23,\n",
    "            'is_14':is_14,\n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:3 of 16000 iterations ended with a divergence (0.0187 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.99 to remove the divergences.\n"
     ]
    }
   ],
   "source": [
    "res_sm03 = sm03.sampling(data=dat_dict, iter=8000,  warmup=4000, thin=1, chains=4,control = dict(adapt_delta=0.99),seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing leave-one-out cross-validation information criterion (LOOIC) for model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\stan\\lib\\site-packages\\arviz\\stats\\stats.py:532: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n"
     ]
    }
   ],
   "source": [
    "idata_sm03 = az.from_pystan(posterior=res_sm03,log_likelihood='log_lik');\n",
    "looic_sm03 = az.loo(idata_sm03,pointwise=True,scale='deviance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
