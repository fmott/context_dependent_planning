{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian Logistic Regression using PyStan\n",
    "**Florian Ott, 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit the three models: Planning (PM), Simple (SM) and Hybrid (HM). Further explanation of the models can be found in the main manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import arviz as az\n",
    "import glob as glob\n",
    "import time as time\n",
    "\n",
    "# Load particpant data \n",
    "filename = glob.glob('../data/behaviour/data_all_partipants_20210623102746.csv') \n",
    "dat = pd.read_csv(filename[0],index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code for the PM and SM \n",
    "standard_m1 = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0,upper=1> response[N];\n",
    "  vector[N] dv;\n",
    "  vector[N] is_basic;\n",
    "  vector[N] is_full_energy;\n",
    "  vector[N] is_low_energy_LC;\n",
    "  vector[N] is_low_energy_HC;\n",
    "  int<lower=0> N_subjects;\n",
    "  int<lower = 1> vpn[N];  \n",
    "}\n",
    "\n",
    "parameters {\n",
    "//hyper parameters\n",
    "  real mu_theta_basic;\n",
    "  real mu_beta_dv;\n",
    "  real<lower=0> sigma_theta_basic;\n",
    "  real<lower=0> sigma_beta_dv;\n",
    "  \n",
    "//parameters \n",
    "  vector[N_subjects] theta_basic;\n",
    "  real theta_full_energy;\n",
    "  real theta_low_energy_LC;\n",
    "  real theta_low_energy_HC;\n",
    "  vector[N_subjects] beta_dv;\n",
    "}\n",
    "\n",
    "model {\n",
    "//hyper priors\n",
    "  mu_theta_basic ~ normal(0,2);\n",
    "  mu_beta_dv ~ normal(0,2);\n",
    "  sigma_theta_basic ~ normal(0,2);\n",
    "  sigma_beta_dv ~ normal(0,2);\n",
    "\n",
    "// priors \n",
    "  theta_basic ~ normal(mu_theta_basic, sigma_theta_basic);\n",
    "  theta_full_energy ~ normal(0, 2);\n",
    "  theta_low_energy_LC ~ normal(0, 2);\n",
    "  theta_low_energy_HC ~ normal(0, 2);\n",
    "  beta_dv ~ normal(mu_beta_dv,sigma_beta_dv);  \n",
    "\n",
    "// likelihood \n",
    "  response ~ bernoulli_logit(theta_full_energy * is_full_energy + theta_low_energy_LC * is_low_energy_LC + theta_low_energy_HC * is_low_energy_HC + theta_basic[vpn] .* is_basic + beta_dv[vpn] .* dv);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  vector[N] response_new;\n",
    "  vector[N_subjects] theta_basic_rep;\n",
    "  vector[N_subjects] beta_dv_rep;\n",
    "\n",
    "// pointwise log-likelihood\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = bernoulli_logit_lpmf(response[n]  |  (theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic[vpn[n]] * is_basic[n] + beta_dv[vpn[n]] * dv[n]));\n",
    "    }\n",
    "\n",
    "// posterior predictive simulation  \n",
    "  for (n in 1:N_subjects){\n",
    "    theta_basic_rep[n] = normal_rng(mu_theta_basic, sigma_theta_basic);\n",
    "    beta_dv_rep[n] = normal_rng(mu_beta_dv, sigma_beta_dv);\n",
    "    }\n",
    "\n",
    "  for (n in 1:N){\n",
    "    response_new[n] = bernoulli_logit_rng(theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_rep[vpn[n]] * is_basic[n] + beta_dv_rep[vpn[n]] * dv[n]);\n",
    "    } \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code for the HM model.\n",
    "# This includes four offer specific biases. \n",
    "standard_m2 = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0,upper=1> response[N];\n",
    "  vector[N] dv;\n",
    "  vector[N] is_basic_1;\n",
    "  vector[N] is_basic_2;\n",
    "  vector[N] is_basic_3;\n",
    "  vector[N] is_basic_4;\n",
    "  vector[N] is_full_energy;\n",
    "  vector[N] is_low_energy_LC;\n",
    "  vector[N] is_low_energy_HC;\n",
    "  int<lower=0> N_subjects;\n",
    "  int<lower = 1> vpn[N];  \n",
    "}\n",
    "\n",
    "parameters {\n",
    "// hyper paramters \n",
    "  real mu_theta_basic_1;\n",
    "  real mu_theta_basic_2;\n",
    "  real mu_theta_basic_3;\n",
    "  real mu_theta_basic_4;\n",
    "  real mu_beta_dv;  \n",
    "  real<lower=0> sigma_theta_basic_1;\n",
    "  real<lower=0> sigma_theta_basic_2;\n",
    "  real<lower=0> sigma_theta_basic_3;\n",
    "  real<lower=0> sigma_theta_basic_4;\n",
    "  real<lower=0> sigma_beta_dv;\n",
    "\n",
    "// parameters\n",
    "  vector[N_subjects] theta_basic_1;\n",
    "  vector[N_subjects] theta_basic_2;\n",
    "  vector[N_subjects] theta_basic_3;\n",
    "  vector[N_subjects] theta_basic_4;\n",
    "  real theta_full_energy;\n",
    "  real theta_low_energy_LC;\n",
    "  real theta_low_energy_HC;\n",
    "  vector[N_subjects] beta_dv;\n",
    "}\n",
    "\n",
    "model {\n",
    "//hyper priors\n",
    "  mu_theta_basic_1 ~ normal(0,2);\n",
    "  mu_theta_basic_2 ~ normal(0,2);\n",
    "  mu_theta_basic_3 ~ normal(0,2);\n",
    "  mu_theta_basic_4 ~ normal(0,2);\n",
    "  mu_beta_dv ~ normal(0,2);  \n",
    "  sigma_theta_basic_1 ~ normal(0,2);\n",
    "  sigma_theta_basic_2 ~ normal(0,2);\n",
    "  sigma_theta_basic_3 ~ normal(0,2);\n",
    "  sigma_theta_basic_4 ~ normal(0,2);\n",
    "  sigma_beta_dv ~ normal(0,2);\n",
    "\n",
    "// priors\n",
    "  theta_basic_1 ~ normal(mu_theta_basic_1,sigma_theta_basic_1);\n",
    "  theta_basic_2 ~ normal(mu_theta_basic_2,sigma_theta_basic_2);\n",
    "  theta_basic_3 ~ normal(mu_theta_basic_3,sigma_theta_basic_3);\n",
    "  theta_basic_4 ~ normal(mu_theta_basic_4,sigma_theta_basic_4);\n",
    "  theta_full_energy ~ normal(0,2);\n",
    "  theta_low_energy_LC ~ normal(0,2);\n",
    "  theta_low_energy_HC ~ normal(0,2);\n",
    "  beta_dv ~ normal(mu_beta_dv,sigma_beta_dv);  \n",
    "\n",
    "// likelihood \n",
    "  response ~ bernoulli_logit(theta_full_energy * is_full_energy + theta_low_energy_LC * is_low_energy_LC + theta_low_energy_HC * is_low_energy_HC + theta_basic_1[vpn] .* is_basic_1 + theta_basic_2[vpn] .* is_basic_2 + theta_basic_3[vpn] .* is_basic_3 + theta_basic_4[vpn] .* is_basic_4 + beta_dv[vpn] .* dv);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  vector[N] response_new;\n",
    "  vector[N_subjects] theta_basic_1_rep;\n",
    "  vector[N_subjects] theta_basic_2_rep;\n",
    "  vector[N_subjects] theta_basic_3_rep;\n",
    "  vector[N_subjects] theta_basic_4_rep;\n",
    "  vector[N_subjects] beta_dv_rep;\n",
    "\n",
    "// pointwise log-likelihood\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = bernoulli_logit_lpmf(response[n]  |  (theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1[vpn[n]] * is_basic_1[n] + theta_basic_2[vpn[n]] * is_basic_2[n] + theta_basic_3[vpn[n]] * is_basic_3[n] + theta_basic_4[vpn[n]] * is_basic_4[n] + beta_dv[vpn[n]] * dv[n]));\n",
    "    }\n",
    "\n",
    "// posterior predictive simulation  \n",
    "  for (n in 1:N_subjects){\n",
    "    theta_basic_1_rep[n] = normal_rng(mu_theta_basic_1, sigma_theta_basic_1);\n",
    "    theta_basic_2_rep[n] = normal_rng(mu_theta_basic_2, sigma_theta_basic_2);\n",
    "    theta_basic_3_rep[n] = normal_rng(mu_theta_basic_3, sigma_theta_basic_3);\n",
    "    theta_basic_4_rep[n] = normal_rng(mu_theta_basic_4, sigma_theta_basic_4);\n",
    "    beta_dv_rep[n] = normal_rng(mu_beta_dv, sigma_beta_dv);\n",
    "    }  \n",
    "\n",
    "  for (n in 1:N){\n",
    "    response_new[n] = bernoulli_logit_rng(theta_full_energy * is_full_energy[n] + theta_low_energy_LC * is_low_energy_LC[n] + theta_low_energy_HC * is_low_energy_HC[n] + theta_basic_1_rep[vpn[n]] * is_basic_1[n] + theta_basic_2_rep[vpn[n]] * is_basic_2[n] + theta_basic_3_rep[vpn[n]] * is_basic_3[n] + theta_basic_4_rep[vpn[n]] * is_basic_4[n] + beta_dv_rep[vpn[n]] * dv[n]);\n",
    "    } \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_430bf6b44e78bc5e7461815b80fab04b NOW.\n"
     ]
    }
   ],
   "source": [
    "sm_standard_m1 = pystan.StanModel(model_code=standard_m1,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_537ce8db94eafff0262fe8a4e5d5afbb NOW.\n"
     ]
    }
   ],
   "source": [
    "sm_standard_m2 = pystan.StanModel(model_code=standard_m2,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2000\n",
    "n_warmup = 1000\n",
    "n_chains = 4\n",
    "param_names_simple = ['theta_full_energy', 'theta_low_energy_LC','theta_low_energy_HC', 'theta_basic','beta_dv']\n",
    "\n",
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic = dat.loc[idx,['is_basic']].to_numpy(dtype='int').squeeze()\n",
    "dv = dat.loc[idx,['dv_simple']].to_numpy().squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "\n",
    "dat_dict_simple = {'N':len(response),         \n",
    "            'response':response,\n",
    "            'dv':dv, \n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic':is_basic,\n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2000\n",
    "n_warmup = 1000\n",
    "n_chains = 4\n",
    "param_names_planning = ['theta_full_energy', 'theta_low_energy_LC','theta_low_energy_HC', 'theta_basic','beta_dv']\n",
    "\n",
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic = dat.loc[idx,['is_basic']].to_numpy(dtype='int').squeeze()\n",
    "dv = dat.loc[idx,['dv_planning']].to_numpy().squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "\n",
    "dat_dict_planning = {'N':len(response),         \n",
    "            'response':response,\n",
    "            'dv':dv, \n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic':is_basic,   \n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2000\n",
    "n_warmup = 1000\n",
    "n_chains = 4\n",
    "param_names_hybrid = ['theta_full_energy', 'theta_low_energy_LC','theta_low_energy_HC', 'theta_basic_1','theta_basic_2','theta_basic_3','theta_basic_4','beta_dv']\n",
    "control_dict = dict(adapt_delta=0.95)\n",
    "\n",
    "idx = (dat['timeout'] == 0)\n",
    "response = (dat.loc[idx,['response']] == 0).to_numpy(dtype='int').squeeze()\n",
    "is_full_energy = dat.loc[idx,['is_full_energy']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_LC = dat.loc[idx,['is_low_energy_LC']].to_numpy(dtype='int').squeeze()\n",
    "is_low_energy_HC = dat.loc[idx,['is_low_energy_HC']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_1 = dat.loc[idx,['is_basic_1']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_2 = dat.loc[idx,['is_basic_2']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_3 = dat.loc[idx,['is_basic_3']].to_numpy(dtype='int').squeeze()\n",
    "is_basic_4 = dat.loc[idx,['is_basic_4']].to_numpy(dtype='int').squeeze()\n",
    "is_basic = dat.loc[idx,['is_basic']].to_numpy(dtype='int').squeeze()\n",
    "dv = dat.loc[idx,['dv_planning']].to_numpy().squeeze()\n",
    "vpn = dat.loc[idx,['vpn']].to_numpy().squeeze() - 100\n",
    "N_subjects = len(np.unique(vpn))\n",
    "\n",
    "dat_dict_hybrid= {'N':len(response),         \n",
    "            'response':response,\n",
    "            'dv':dv,      \n",
    "            'is_full_energy':is_full_energy ,\n",
    "            'is_low_energy_LC':is_low_energy_LC,\n",
    "            'is_low_energy_HC':is_low_energy_HC,\n",
    "            'is_basic_1':is_basic_1,\n",
    "            'is_basic_2':is_basic_2,\n",
    "            'is_basic_3':is_basic_3,\n",
    "            'is_basic_4':is_basic_4,\n",
    "            'is_basic':is_basic,\n",
    "            'N_subjects':N_subjects,\n",
    "            'vpn':vpn\n",
    "            } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "res_simple = sm_standard_m1.sampling(data=dat_dict_simple, iter=n_iter,  warmup=n_warmup, thin=1, chains=n_chains,seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "res_planning = sm_standard_m1.sampling(data=dat_dict_planning, iter=n_iter,  warmup=n_warmup, thin=1, chains=n_chains,seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "res_hybrid = sm_standard_m2.sampling(data=dat_dict_hybrid, iter=n_iter,  warmup=n_warmup, thin=1, chains=n_chains,control = control_dict,seed=101, verbose = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing leave-one-out cross-validation information criterion (LOOIC) for model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Stan fit-objects to Arviz inference data\n",
    "idata_simple = az.from_pystan(posterior=res_simple,log_likelihood='log_lik');\n",
    "idata_planning = az.from_pystan(posterior=res_planning,log_likelihood='log_lik');\n",
    "idata_hybrid = az.from_pystan(posterior=res_hybrid,log_likelihood='log_lik');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOOIC\n",
    "looic_simple = az.loo(idata_simple,pointwise=True,scale='deviance')\n",
    "looic_planning = az.loo(idata_planning,pointwise=True,scale='deviance')\n",
    "looic_hybrid = az.loo(idata_hybrid,pointwise=True,scale='deviance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
